---
title: "Bayesian HW5"
author: "Kerui Cao"
date: "11/10/2019"
output: pdf_document
---
```{r setup,echo=F}
knitr::opts_chunk$set(echo = T,out.width="0.9\\linewidth",dev="png",fig.align  = 'center',warning = F,message = F,dpi = 500)
pacman::p_load(tidyverse,knitr,arm,foreign,gridExtra,magrittr,dplyr,tidyr,rstan,bayesplot)
```


```{r}
data = data.frame("y" = c(13,52,6, 40, 10, 7, 66, 10, 10, 14, 16, 4,
                         65, 5, 11, 10, 15, 5, 76, 56, 88, 24, 51, 4,
                         40, 8, 18, 5, 16, 50, 40, 1, 36, 5, 10, 91,
                         18, 1, 18, 6, 1, 23, 15, 18, 12, 12, 17, 3))
```

```{r}
y = data$y
mcmc_array <- function (ns, nchains = 1, params) {
  nparams <- length(params)
  array(dim = c(ns, nchains, nparams),
        dimnames = list(iterations = NULL,
                        chains = paste0("chain", 1:nchains),
                        parameters = params))
}

nc = 4
ns = 10000
cs = c("Lambda","Mu","Sigma","Lp__")

la.c = 0.5

la.sd = 0.01

n = length(y)

sims = mcmc_array(ns,nchains = nc, params = cs)

wi = function(la){
  y = data$y
  if(abs(la)<0.005){
    w = log(y)
  } else{
    w = (y^la-1)/la
  }
  return(w)
}

w.mu = function(la,mu){
  w = wi(la = la)
  re = 0-sum((w-mu)^2)
  return(re)
}

mean.mu = function(la){
  w = wi(la = la)
  return(mean(w))
}

si.n = function(si){
  return(si^(-(n+1)/2))
}
for(j in 1:nc){
  for(i in 1:ns){
    mu.c = rnorm(n = 1,mean = mean.mu(la = la.c),sd = si.c/n)
    si.c = 1/rgamma(n = 1,shape = (n-1)/2,rate = -w.mu(la = la.c,mu = mu.c)/2)
    la.s = rnorm(n = 1,mean = la.c,sd = la.sd)
    up = sum((la.s-1)*log(y))+w.mu(la = la.s,mu = mu.c)/(2*si.c)
    down = sum((la.c-1)*log(y))+w.mu(la = la.c,mu = mu.c)/(2*si.c)
    r1 = exp(up-down)
    if(r1 >= runif(1)){la.c = la.s}else{la.c = la.c}
    lp = -0.5*log(si.c) + sum(log(dnorm(wi(la = la.c),mean = mu.c,sd = si.c))) + sum((la.c-1)*log(y))
    sims[i,j,] = c(la.c,mu.c,si.c,lp)
  }
}
```

```{r}
monitor(sims[,c(1,3),])
sim = sims[5000:10000,,]
mcmc_trace(sim)
mcmc_acf(sim)
mcmc_dens_overlay(sim)
mcmc_hist(sim)
```
```{r}
quantile(sim[,,1],c(0.025,0.975))
quantile(sim[,,2],c(0.025,0.975))
quantile(sim[,,3],c(0.025,0.975))
```

\qquad So the 95% Confidence interval is

+ $\lambda$ : [-0.0668,0.4698]
+ $\mu$ : [2.3735,6.1785]
+ $\sigma$ : [0.9176,17.7471]

```{r}
ss = n
lambda = sim[,,1]
mu = sim[,,2]
sigma = sim[,,3]
la.sa = sample(x = lambda,size = ss)
mu.sa = sample(x = mu,size = ss)
si.sa = sample(x = sigma,size = ss)
w = rnorm(n = ss,mean = mu.sa,sd = si.sa)
y = array(dim = ss)
for(i in 1:ss){
  if(abs(la.sa[i])<0.005){y[i] = exp(w[i])}else{
    y[i] = (w[i]*(la.sa[i]+1)^(la.sa[i]))
  }
}
sam = data.frame(cbind(y,w,la.sa,mu.sa,si.sa))
#sam %<>% filter(y>0)
com = data.frame("pre" = sam$y,"data" = data$y) %>% filter(pre >0)
quantile(com$pre,c(0.025,0.975))
com = pivot_longer(data = com,cols = colnames(com),names_to = "type", values_to = "value")
com <- com %>%
 filter(value >= 0L & value <= 50L)

ggplot(com) +
 aes(x = type, y = value) +
 geom_boxplot(fill = "#0c4c8a") 
```

\qquad The 95% Confidence Interval for $\tilde{y}$ is [0.0707,40.4988]

\qquad From the boxplot, we can see that the predicted $\tilde{y}$ has obvious lower mean and variance.  