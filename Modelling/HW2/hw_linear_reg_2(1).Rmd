---
title: "Homework 02"
author: "yourname"
date: "Septemeber 16, 2018"
output:
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
pacman::p_load("arm",
               "data.table",
               "Cairo",
               "faraway",
               "foreign",
               "tidyverse",
               "knitr",
               'VIM',
               "kableExtra")
opts_chunk$set(echo = F,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE,
               warning = FALSE, message=FALSE,dpi = 346)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

## Data analysis 

### Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"),warn.missing.labels = T)
```

Pull out the data on earnings, sex, height, and weight.  

#### 1.In R, check the dataset and clean any unusually coded data.
```{r echo=FALSE, fig.height=3.3}
info=NULL
for(i in 1:length(names(heights))){
  num_of_uniquevalue=length(unique(heights[,i]))
  Na = sum(is.na(heights[,i]))
  Ze = sum(heights[,i]==0,na.rm =T)
  P_na = (Na+Ze)/length(heights[,i])
  info = cbind(info,matrix(c(num_of_uniquevalue,Na,Ze,P_na),ncol = 1))
}
rownames(info)=c('Number of Unique Value','Number of Missing Value','Number of 0','% of Missing Value')
colnames(info)=names(heights)
kable(info,caption = "Quality of Data",align = "c",digits = 2,
      format = 'latex',booktabs=T,longtable=T) %>% 
  kable_styling(font_size = 8,bootstrap_options = 
                  c('striped','hover','condensed',"responsive"))
matrixplot(heights)
```
\qquad Above chart and figure show the distribution of missing value and 0 amoung variables, in the figure, the red means there is a missing value, and for the rest, the darker the color, the higher the value.  

\qquad So according to the analysis above, the most of the missing value ,which is represented by 'NA' and '0', contained in variable "earning", about $41\%$ of data doesn`t have variable 'earning', which is reasonable, beacause earning is kond of a private quetion, due to the extent of missing, we can hardly apply any imputaion, so simply delete it.  
```{r fig.height=3.3}
for(i in 1:length(names(heights))){
  heights[which(heights[,i]==0),i]=NaN
}
hi = na.omit(heights)
info=NULL
for(i in 1:length(names(hi))){
  num_of_uniquevalue=length(unique(hi[,i]))
  Na = sum(is.na(hi[,i]))
  Ze = sum(hi[,i]==0)
  P_na = (Na+Ze)/length(hi[,i])
  info = cbind(info,matrix(c(num_of_uniquevalue,Na,Ze,P_na),ncol = 1))
}
rownames(info)=c('Number of Unique Value','Number of Missing Value','Number of 0','% of Missing Value')
colnames(info)=names(hi)
kable(info,caption = "Quality of Data",digits = 2,
      align = "c",format = 'latex',booktabs=T,longtable = T) %>% 
  kable_styling(font_size = 8,bootstrap_options = 
                  c('striped','hover','condensed',"responsive"))
matrixplot(hi)
```
\qquad So now the data is clean and ready to be analysisd.  

#### 2.Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model as average earnings for people with average height?  

\qquad Look at the data, we can find three variables that related to height, "height 1", "height 2" and "height", after reading the original article, we know that real height should be "hight 1" feet and "height 2" inches, so $Real Height=12*height_1+height_2$, which is exactly the variable "height".  
```{r}
hi[,'h_over_m']=hi$height - mean(hi$height)
ggplot(hi,aes(y=earn,x = h_over_m)) +
  geom_point() + geom_smooth(se=F)
```  
\qquad Look at the plot of height over average against earning, we can easily find some outliers, and a linear relationship may not apply to this situation, so we apply some non-linear transformation, first we consider use $\log(earn)$ replace earn to eliminate the effect of outliers.  As for the predictor, we try to include the polynomial in our model.  
```{r fig.width= 8}
pnl=ggplot(hi,aes(y=earn,x = h_over_m)) +
  geom_point() + geom_smooth(se=F)
pl=ggplot(hi,aes(y=log(earn,10),x = h_over_m)) +
  geom_point() + geom_smooth(se=F)
gridExtra::grid.arrange(pnl,pl,ncol=2)
```
\qquad From above plot, we can tell that replacing earn by $\log_{10}Earn$ does comdence the data and alleviate the effect of outliers a bit. Than apply regression analysis.  
```{r}
mo = lm(log(earn,10)~h_over_m+I(h_over_m^2)+I(h_over_m^3),data = hi)
summary(mo)
car::qqPlot(mo$residuals, id=F)
```
\qquad From result above, we can see that the performance of the model is poor, whose residuals are not normally distributed and even not close. I tried different combination of predictors, including heighs and the power of height. The coefficience of model shows that only the coefficience of height is significant, Which means one unit increase(1 inch) will increase the earn by around $2.8\%$.  

#### 3. Fit some regression models with the goal of predicting earnings from some combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.  

\qquad To select proper predictors, we first do some analysis.  
```{r}
cor = reshape2::melt(cor(hi))
ggplot(cor,aes(y=Var1,x=Var2,fill=value)) + 
  geom_tile()+
  scale_fill_gradient2(name="Value",low = "darkgreen",high = "red", mid = "white")
```
\qquad We can tell that sex is highly related to height, earn is related to earn, race is not obviously related to any othor variables. So we may consider put sex, ed into our model.  

\qquad Before we analyze sex, we have to transform sex into a binary variable, where 0 represent male, 1 represent female.  
```{r}
hi$sex=hi$sex-1
```

```{r fig.height=6}
aa1 = ggplot(hi,aes(y=log(earn,10),x=h_over_m,color = as.factor(sex))) + geom_point() + geom_smooth(method = 'lm') + theme(legend.position = 'top')
aa2 = ggplot(hi,aes(y=log(earn,10),x = as.factor(sex))) + geom_boxplot()
gridExtra::grid.arrange(aa1,aa2,ncol=1)
```
\qquad For sex, we can reasonablly guess that sex does affect the average of earning and the way height affecting earn.  
```{r}
ggplot(hi,aes(y=log(earn,10),x=ed,color = as.factor(sex))) + geom_point() + geom_smooth() + theme(legend.position = 'top')
```
\qquad We can tell the eduacation level is related to earn, rough judge from above figure, we can tell that higher education level, higher earning, and sex has no obvious effect on the slope of education level.  
```{r fig.height=3.8}
mu = lm(log(earn,10)~h_over_m+sex+I(log(ed))+sex:h_over_m+I(log(ed)*sex),data = hi)
summary(mu)
ggplot()+geom_histogram(aes(x=mu$residuals))
car::qqPlot(mu$residuals, envelope = 0.99,grid=F, id=F)
```
\qquad After adding new variables to our model, $R^2$ increased a little, but the residuals of model is still not normally distributed.  

#### 4. Interpret all model coefficients.

\qquad Clearly, this model is bad, low $R^2$ means this model could exlpain little of the variance of output, and half of the coefficients are not significant, but due to some of them are highly correlated, we have to keep them to control the influence of endogeneity. Here is the interpretation of coefficients:  

\qquad h_over_m and h_over_sex: when keeping others unchanged, one unit higher than average height, earning will increase around $0.863\%$ for male, onthe contrary, will decrease around $0.529\%$  

\qquad sex: on average, keeping others unchanged, earning of female will be around 48% lower than male;  

\qquad I(log(ed)) and I(log(ed)): on average, keeping others unchanged, $1\%$ increase in education level, for male, earning will increase around $50\%$, for female, earning will increase $73\%$;  

#### 5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
kable(confint(mu,level = 0.95),align = 'c',caption = "Confident Interval of Coefficients",booktab = T,format = 'latex') %>% kable_styling(latex_options = "hold_position",bootstrap_options = c('striped','hover','condensed',"responsive"))
```

\qquad $95\%$ Confident Interval means that each interval has $95\%$ chance that contain real coefficient.  


### Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO@    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

#### 1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r fig.height = 3}
ggplot(data = pollution,aes(y = mort/100, x = nox)) + geom_point() + geom_smooth()
```

\qquad From above plot we can tell that linear relationship between mortality rate and nitric oxides won`t fit well, and we can also tell from the smooth line that two outliers does influent the result.  


#### 2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r fig.height = 2.8}
ggplot(data = pollution[which(pollution$nox<100),],aes(y = mort/100, x = nox)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F) + theme(legend.position = 'top')
```

\qquad Exclude those two outliers, we can tell that we may try to add $nox^3$ to the predictors.  

```{r}
m = lm(data = pollution, mort~nox+I(nox^2)+I(nox^3))
summary(m)
```

\qquad As we can see, the coefficients of the power of nox are all significant, but due to the small number of predictors, the $R^2$ of model is quite small.  

```{r fig.height = 3.3}
ggplot() + geom_histogram(aes(x =m$residuals),bins = 18)
car::qqPlot(m$residuals,id=F)
```

\qquad Even the $R^2$ is small, but the residuals comply with normal distribution.  

#### 3. Interpret the slope coefficient from the model you chose in 2.

\qquad In question 2, the regression model is $mort=914.2+2.582\cdot nox-0.02468\cdot nox^2+0.00005048\cdot nox^3$, the derivative on nox is $2.582-0.04936\cdot nox+0.00015144\cdot nox^2$, which means, assume $nox=1$, every unit increase of nox, motality rate increase by 2.253791, and from the figure below we can tell that the marginal increase of mort is decrease.  

```{r fig.width= 7, fig.height = 3.5}
par(mfrow=c(1,2))
curve(914.2+2.582*x-0.02468*x^2+0.00005048*x^3, from = 0,to = 60, n = 100,ylab = "Mort")
curve(2.582-0.04936*x+0.00015144*x**2, from = 0,to = 60, n = 100,ylab = "Derivative on Nox")
```

#### 4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
kable(confint(m,level = 0.99),align = 'c',caption = "Confident Interval of Coefficients",booktab = T,format = 'latex') %>% kable_styling(latex_options = "hold_position",bootstrap_options = c('striped','hover','condensed',"responsive"))
```

#### 5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when helpful. Plot the fitted regression model and interpret the coefficients.

```{r fig.height=3.3}
p1 = ggplot(data = pollution,aes(y = mort, x = so2)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F)
p2 = ggplot(data = pollution[which(pollution$hc<200),],aes(y = mort, x = hc)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F)
gridExtra::grid.arrange(p1,p2,ncol=1)
```

\qquad We can tell that for sulfur dioxide, linear relationship with mortality rate is proer, but for hydrocarbons, a transformation to the power of hydrocarbons is necessary.  
```{r}
mm = lm(mort~nox+I(nox^3)+I(so2^(-1))+I(hc^2),data = pollution)
summary(mm)
car::qqPlot(mm$residuals,id=F)
```

\qquad According to the result of the model, the residuals comply with normal distribution, the coefficients of the power of nox and hc are significant, but the coefficient of so2 are not even I have tried several diffrent transformation, but due to the correlation between nox, so2 and hc is high (nox~so2:`r cor(pollution$nox, pollution$so2)`), it`s better to delete this variables so that the coefficients of nox and hc will not be affected.  

#### 6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}
library(gridExtra)
mmm = lm(mort~nox+I(nox^3)+I(so2^(-1))+I(hc^2),data = pollution[1:30,])
pre = data.frame(predict(object = mmm, newdata = pollution[31:60,],interval = 'confidence', level = 0.95))
aberr = data.frame(cbind(1:30,abs(pre$fit - pollution$mort[31:60])))
pl = cbind(1:30,pre,pollution$mort[31:60])
colnames(pl)=c('index','predicted','lower level','upper level','real value')
colnames(aberr)=c('index','Absolute_Error')
pl = data.frame(pl)
pl = tidyr::gather(pl,'type','value',2:5)
pp1 = ggplot(data = pl) +xlab('')+ylab('Mort')+ geom_line(aes(y = value, x = index, color = type)) + theme(legend.position = 'top')
pp2 = ggplot(data = aberr) + geom_col(aes(y=Absolute_Error, x = index)) + xlab("")
grid.arrange(pp1,pp2,ncol=1,nrow=2)
```

### Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
```

1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.

```{r fig.height=10}
g1 = ggplot(teengamb,aes(y=gamble,x = status)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F) +theme(legend.position = 'top') + ylab("")
g2 = ggplot(teengamb,aes(y=gamble,x = income)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F) +theme(legend.position = 'top')+ ylab("")
g3 = ggplot(teengamb,aes(y=gamble,x = verbal)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F) +theme(legend.position = 'none')+ ylab("")
g4 = ggplot(teengamb) + geom_boxplot(aes(y=gamble,x = as.factor(sex)))+ ylab("")
g5 = ggplot(teengamb,aes(y=gamble,x = status,color = as.factor(sex))) + geom_point() + geom_smooth(method = 'lm',se =F) + theme(legend.position = 'top') + ylab("") 
g6 = ggplot(teengamb,aes(y=gamble,x = income,color = as.factor(sex))) + geom_point() + geom_smooth(method = 'lm',se =F) + theme(legend.position = 'none') + ylab("")
g7 = ggplot(teengamb,aes(y=gamble,x = verbal,color = as.factor(sex))) + geom_point() + geom_smooth(method = 'lm',se =F) + theme(legend.position = 'none') + ylab("")
grid.arrange(arrangeGrob(g1,g2,g3,g4,ncol = 2),g5,g6,g7,ncol=1,heights = c(2.5,1,1,1))
```

\qquad According to figure above, we consider adding the power of status and verbal as predictors, and also we may consider the interaction between sex and other predictors.
```{r fig.height=3.5}
sm = lm(data = teengamb, gamble~status + I(status^2)+income+verbal+sex+sex:status+sex:income)
summary(sm)
car::qqPlot(sm$residuals,envelope = 0.95,id=F)
```

\qquad From above result, most of the coefficients are significant, and residuals comply with normal distribution. Coefficients interpretatoin:  
*status and status:sex*: because the coefficient of the power of status is too small compared to the coefficient of status, so we can ignore it, so the coefficient of status means that one unit change in social status, keeping others unchanged, the average expenditure on gambling per year will decrease around 3.5 pound for male, for female, it will only decrease 2.42 pounds;  

*income and oncome:sex*: one pound increase of weekage, keeping others unchaged, expenditure on gambling will increase by 5.7 pounds for male, for female, it will decrease by 0.1 pounds;  

*verbal*: one unit increase in verbal score, keeping others unchanged, expenditure on gambling will decrease by 0.9 pounds;  

*sex*: the average expenditure on gambling for female is 43 pounds less than male.  

2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}
kable(confint(sm,level = 0.99),align = 'c',caption = "Confident Interval of Coefficients",booktab = T,format = 'latex') %>% kable_styling(latex_options = "hold_position",bootstrap_options = c('striped','hover','condensed',"responsive"))

```

3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
me = matrix(c(0,mean(teengamb$status),mean(teengamb$income),mean(teengamb$verbal),0),nrow = 1)
mx = matrix(c(0,max(teengamb$status),max(teengamb$income),max(teengamb$verbal),0),nrow = 1)
p = rbind(me,mx)
colnames(p)=c('sex','status','income','verbal','gemble')
pp = data.frame(predict(sm,newdata = data.frame(p), interval = "confidence",level = 0.95))
pp$variate = pp$upr - pp$lwr
rownames(pp)=c('Mean level','max level')
kable(pp,align = 'c',caption = "prediction of gamble",booktab = T,format = 'latex') %>% kable_styling(latex_options = "hold_position",bootstrap_options = c('striped','hover','condensed',"responsive"))
```

\qquad From above result, the CI of prediction using max level is bigger.  

### School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
```

1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors.  Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
d1 = ggplot(sat,aes(y=total,x = expend)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F) +theme(legend.position = 'right') + ylab("")
d2 = ggplot(sat,aes(y=total,x = ratio)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F) +theme(legend.position = 'right') + ylab("")
d3 = ggplot(sat,aes(y=total,x = salary)) + geom_point() + geom_smooth(aes(colour="non-linear fit"),se =F) + geom_smooth(method = 'lm',aes(colour="linear fit"),se =F) +theme(legend.position = 'right') + ylab("")
grid.arrange(d1,d2,d3,ncol=1)
```

\qquad we can tell from the above figure, that a linear relationship may be enough.  
```{r}
mat = lm(data = sat, total~expend+ratio+salary)
summary(mat)
```

\qquad Clearly that the SAT score has nothing to do with expend and ratio, but salary has some influence on sat score. Coefficients interpretation:  

*expend*: One unit increase in expend may increase the sat score by 11.4, keeping others unchanged;  

*ratio*: One unit increase in ratio may decrease the Sat score by 0.436, keeping other unchanged;  

*salary*: Due to the coefficient of $salary^2$ is too small compared to the coefficient of salary, we just ignore the coefficient of $salary^2$, so the coefficient of salary means that one unit increase insalary will decrease the SAT score by 19.06.  

2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
kable(confint(mat,level = 0.98),align = 'c',caption = "Confident Interval of Coefficients",booktab = T,format = 'latex',digits = 2) %>% kable_styling(latex_options = "hold_position",bootstrap_options = c('striped','hover','condensed',"responsive"))

```

3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
mat2 = lm(data = sat, total~expend+ratio+salary+takers)
summary(mat2)
```

\qquad Adding takers into the model improves the performance of this model a lot, mainly represented by the bigger $R^2$, which increases $61\%$.  

# Conceptual exercises.

### Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

* The simple difference, $D_i-R_i$

* The ratio, $D_i/R_i$

* The difference on the logarithmic scale, $log D_i-log R_i$ 

* The relative proportion, $D_i/(D_i+R_i)$.


### Transformation 

For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?

\qquad If we transform $x$ into $x-10$, $\hat{\alpha}^{\star}=\hat{\alpha}+10\times\hat{\beta}=1+10\times0.9=91$, $\hat{\beta},\hat{\sigma},r$ remains.  

\qquad If we we change $x$ into $10x$, $\hat{\beta}^{\star}=\frac{\hat{\beta}}{10}=9\times10^{-2},\hat{\sigma}^{\star}=\frac{\hat{\sigma}}{10^2}=3\times10^{-4}$, $\hat{\alpha},r$ remains.  

\qquad If we transform $x$ into $10(x-1)$, $\hat{\alpha}^{\star}=91,\hat{\beta}^{\star}=9\times10^{-2},\hat{\sigma}^{\star}=3\times10^{-4}$, $r$ remains.


2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?


3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.


5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.


6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.
